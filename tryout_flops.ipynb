{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptflops import get_model_complexity_info\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from pruneshift.networks import network\n",
    "from pruneshift.prune import prune\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(torch.tensor([1, 2.]).mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "floats = []\n",
    "for \n",
    "net = network(\"cifar100_resnet50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune(net2, \"global_weight\", 64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10722134.0, 23705252)\n"
     ]
    }
   ],
   "source": [
    "# We can pass custom module hooks to allow specific calculation for pruned modules.\n",
    "# Write a custom function wrapper to scale all the conv_counter.\n",
    "\n",
    "net1 = network(\"cifar100_resnet50\")\n",
    "net2 = network(\"cifar100_resnet50\")\n",
    "\n",
    "\n",
    "def consider_pruning(flops: int, module: nn.Module, param_name: str) -> int:\n",
    "    if not hasattr(module, param_name + \"_mask\"):\n",
    "        factor = 1.\n",
    "    else:\n",
    "        factor = getattr(module, param_name + \"_mask\").mean().item()\n",
    "\n",
    "    return int(factor * flops)\n",
    "\n",
    "\n",
    "def conv_flops_counter_hook(conv_module, input, output):\n",
    "    # Can have multiple inputs, getting the first one\n",
    "    input = input[0]\n",
    "\n",
    "    batch_size = input.shape[0]\n",
    "    output_dims = list(output.shape[2:])\n",
    "\n",
    "    kernel_dims = list(conv_module.kernel_size)\n",
    "    in_channels = conv_module.in_channels\n",
    "    out_channels = conv_module.out_channels\n",
    "    groups = conv_module.groups\n",
    "\n",
    "    filters_per_channel = out_channels // groups\n",
    "    conv_per_position_flops = int(np.prod(kernel_dims)) * \\\n",
    "        in_channels * filters_per_channel\n",
    "\n",
    "    active_elements_count = batch_size * int(np.prod(output_dims))\n",
    "\n",
    "    overall_conv_flops = conv_per_position_flops * active_elements_count\n",
    "    overall_conv_flops = consider_pruning(overall_conv_flops, conv_module, \"weight\")\n",
    "\n",
    "    bias_flops = 0\n",
    "\n",
    "    if conv_module.bias is not None:\n",
    "        bias_flops = out_channels * active_elements_count\n",
    "        # Modification!\n",
    "        bias_flops = consider_pruning(bias_flops, conv_module, \"bias\")\n",
    "\n",
    "    overall_flops = overall_conv_flops + bias_flops\n",
    "\n",
    "    conv_module.__flops__ += int(overall_flops)\n",
    "\n",
    "\n",
    "def linear_flops_counter_hook(module, input, output):\n",
    "    input = input[0]\n",
    "    # pytorch checks dimensions, so here we don't care much\n",
    "    output_last_dim = output.shape[-1]\n",
    "    if module.bias is not None:\n",
    "        bias_flops = consider_pruning(output_last_dim, module, \"bias\")\n",
    "    else:\n",
    "        bias_flops = 0\n",
    "        \n",
    "    weight_flops = int(np.prod(input.shape)) * output_last_dim\n",
    "    weight_flops = consider_pruning(weight_flops, module, \"weight\")\n",
    "    \n",
    "    module.__flops__ += weight_flops + bias_flops\n",
    "\n",
    "\n",
    "def get_model_complexity_prune(model, input_res, print_per_layer_stat=False, as_strings=False):\n",
    "    \"\"\" Calculates the model complexity taking into account pruning in conv2d and linear layers.\"\"\"\n",
    "    custom_modules_hooks = {nn.Conv2d: conv_flops_counter_hook,\n",
    "                           nn.Linear: linear_flops_counter_hook}\n",
    "    return get_model_complexity_info(model, input_res, print_per_layer_stat, as_strings=as_strings, custom_modules_hooks=custom_modules_hooks)\n",
    "\n",
    "# print(get_model_complexity_info(net1, (3, 32, 32), False))\n",
    "print(get_model_complexity_prune(net2, input_res=(3, 32, 32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5264.0, 130)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = nn.Conv2d(3, 10, 2)\n",
    "\n",
    "get_model_complexity_prune(layer, (3, 10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict()"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer._forward_hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Theseus",
   "language": "python",
   "name": "theseus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
